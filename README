TODO:

next steps:

***
have compute_sfs, log_likelihood_prf,
take in a ms_cmd,
instead of a demography


***
remove @memoize_instance?

***
make it so that sfs_entries() always returns branch length


***
find minimal example test case for autograd issue #24


***
rename my_einsum to be einsum

***
change everything to be in tensor format for data
(don't use SumProduct.p(), use sfs_entries() instead)

***
change input format of data
also don't have demography store it anymore
(REMOVE demograph.update_state)

***
make a single function

sfs

which takes in:
data, ms_cmd, theta, params

returns:
sfs, normalization constant

***

is summing the antidiagonals equivalent to doing:
fft along the two dimensions
taking ifft of diagonal



***
make SumProduct entirely tensorized!!!
so "dataset" is an axis, and each index along this axis is a separate configuration

this will make the reverse-order differentiation maximally efficient

***
get rid of LabeledAxisArray,

instead just pass
(tensor, {pop: idx})

tensor is of probabilities, with 0th index=dataset


***
fix
  File "/Users/jackkamm/Dropbox/research/momi/parse_ms.py", line 29, in toFloat
    raise Exception("nan in %s,%s" % (ms_cmd))
TypeError: not enough arguments for format string


***
merge some of my_functions into autograd package with pull request


***
add back numerical stability assertions, etc.
(things seem a little more unstable right now in the gradient descent)


***
implement expm_multiply for autograd branch

***
Change exponential size history to take in alpha
(this way we won't have as many overflow issues if N_top = 0 or inf)

Also make it so that, if -eg 0.0, (and not -eg $0 with $0==0.0),
create a ConstantSize always.

Make tau=inf, tau=0 work for ExpHist
or, assert that tau < inf for any Exponential history

***
Test if autodifferentiation works for functional size history

***
Try expokit stuff in expokit branch

***
copy assertions in ConstantTruncatedSizeHistory.etjj to other etjj



***
Two sources of numerical error:

1) FFT convolution: when vector has a 0.0 sometimes small negative numbers result
however, in our case the 0.0 is always at the beginning or end of vector
so can make FFT convolution more robust by truncating ends as necessary

2) matrix exponential when t is very small, and v has 0s in it
seems like the scipy.sparse.linalg.expm_multiply might be more robust in this case
investigate the timing too



***
make exponential size history work with growth rate 0, and also tau=0.
Fix parse_ms TODO: for tau==0.0 case

***
for parse ms, let -es specificy the label of the new population
this will be required to avoid confusion over population labels when fitting times of events

***
compare which version of expm_multiply is faster (with and without derivatives)


***
make size_history store the SFS as an array,
instead of having sum_product have to convert to an array everytime

***
Make functional size history work with autodifferentiation! Right now assumes it's a constant function always

***
be consistent with leaves vs leafs (leaves is demography property, leafs is argument to parse_ms)

***
get rid of from_newick, to_newick in parse_ms?

***
for simulate_sfs, make theta=None work when -r is set
(right now it counts up the total branch length, but doesn't know
how to deal with multiple trees)

***
replace cached_property module (v1.0.0 not compatible with using nx.DiGraph)
***
make state of derived counts, a property of SumProduct, instead of Demography
***
don't make n_max a field of size_history;
instead have etjj and sfs take n as a parameter, and return
the appropriate vector

***
add option to simulate from ms instead of scrm

***
test in what regime expm_multiply is better than eigenvalue
internally decide which one to use based on the regime


***
rename SumProduct.p()

***
improve the syntax for SFS.
use dictionary for state instead of tuple? (downside: won't be hashable)
store sumCounts, sumSqCounts, nonzeroCounts in a single object?


***
in size_history fix TODO (make deep copy)

***
change everything to be in ms units