#+PROPERTY: header-args:ipython :session :results raw drawer :kernel momi2-gaxri9wx
* Tutorial
  :PROPERTIES:
  :CUSTOM_ID: tutorial
  :END:

This is a tutorial for the =momi= package. You can run the ipython
notebook that created this tutorial at =examples/tutorial.ipynb=.

To get started, import the =momi= package:

#+BEGIN_SRC ipython
    import momi
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[1]:
:END:

Some =momi= operations can take awhile complete, so it is useful to turn
on status monitoring messages to check that everything is running
normally. Here, we output logging messages to the file =tutorial.log=.
See the [[https://docs.python.org/3/library/logging.html][logging]]
module for more details.

#+BEGIN_SRC ipython
  import logging
  #logging.basicConfig(level=logging.INFO, filename="tutorial.log")
  logging.basicConfig(level=logging.INFO)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[2]:
:END:

** Constructing a demographic history
   :PROPERTIES:
   :CUSTOM_ID: constructing-a-demographic-history
   :END:

We will construct a demographic model loosely based on human history. To
start, we need to specify =N_e=, which is the /diploid/ size of each population
unless specified otherwise, and =gen_time=, the amount of
time per generation.

We will measure time in years; the human generation time is
approximately =gen_time=29= years per generation. The human effective
population size is typically measured as $1.2 \times 10^{4}$ so we set
=N_e=1.2e4=.

(Note the population size here is NOT the census size, but the
population genetics concept of
[[https://en.wikipedia.org/wiki/Effective_population_size][effective
population size]] or $N_e$).

#+BEGIN_SRC ipython
  model = momi.DemographicModel(N_e=1.2e4, gen_time=29,
                                muts_per_gen=1.25e-8)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[3]:
:END:


We now specify the sampled ("leaf") populations of our demography. We
specify three populations: YRI, CHB, and NEA. YRI and CHB are
present-day populations at $t=0$, while NEA is an archaic population
from $t=50,000$ years ago. We set YRI and NEA to have the default
population sizes, and set CHB to have a size of $N=100,000$ growing at
rate $.0005$ per year.

TODO check is growth rate g or g/2

#+BEGIN_SRC ipython
    # add YRI leaf at t=0 and default_N
    model.add_leaf("YRI")
    # add  CHB leaf at t=0, N=1e5, growing at rate 5e-4 per unit time (year)
    model.add_leaf("CHB", N=1e5, g=5e-4)
    # add NEA leaf at 50kya
    model.add_leaf("NEA", t=5e4) 
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[4]:
:END:

Demographic events are added to the model by the methods =set_size()=
and =move_lineages()=. =set_size()= is used to change population size
and growth rate, while =move_lineages()= is used for population split
and admixture events.

#+BEGIN_SRC ipython
    # stop CHB growth at 10kya
    model.set_size("CHB", g=0, t=1e4)

    # at 45kya CHB receive a 3% pulse from GhostNea
    model.move_lineages("CHB", "GhostNea", t=4.5e4, p=.03)
    # at 55kya GhostNea joins onto NEA
    model.move_lineages("GhostNea", "NEA", t=5.5e4)

    # at 80 kya CHB goes thru bottleneck
    model.set_size("CHB", N=100, t=8e4)
    # at 85 kya CHB joins onto YRI
    model.move_lineages("CHB", "YRI", t=8.5e4)

    # at 500 kya YRI joins onto NEA
    model.move_lineages("YRI", "NEA", t=5e5)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
:END:

If you are familiar with the software
[[http://home.uchicago.edu/rhudson1/source/mksamples.html][ms]],
=set_size()= is analogous to the flags "-en" and "-eg", while
=move_lineages()= is analogous to the flags "-ej" and "-es". If you are
familiar with the package
[[https://msprime.readthedocs.io/en/stable/][msprime]], then
=set_size()= is analogous to =msprime.PopulationParametersChange=, while
=move_lineages()= is analogous to =msprime.MassMigration=.

Note that events can involve other populations aside from the 3 sampled
populations YRI, CHB, and NEA. Unsampled populations are also known as
"ghost populations". In this example, CHB receives a small amount of
admixture from a population "GhostNea", which splits off from NEA at an
earlier date.

** Plotting a demography
   :PROPERTIES:
   :CUSTOM_ID: plotting-a-demography
   :END:

#+BEGIN_SRC ipython
  yticks = [1e4, 2.5e4, 5e4, 7.5e4, 1e5, 2.5e5, 5e5, 7.5e5]
  fig = momi.DemographyPlot(model, ["YRI", "CHB", "GhostNea", "NEA"],
                            figsize=(6,8),
                            major_yticks=yticks,
                            linthreshy=1e5, pulse_bounds=(0,.25))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[36]:
[[file:./obipy-resources/1676TKl.png]]
:END:

Note the user needs to specify the order of all populations (including
ghost populations) along the x-axis.

The argument =linthreshy= is useful for visualizing demographic events
at different scales. In our example, the split time of NEA is far above
the other events. Times below =linthreshy= are plotted on a linear
scale, while times above it are plotted on a log scale.

*** Additional plotting details
    :PROPERTIES:
    :CUSTOM_ID: additional-plotting-details
    :END:

If you are using Python outside a notebook environment, e.g.Â in a shell,
then you need to call the following before making calls to =draw()=:

#+BEGIN_EXAMPLE
    # call first if in a shell not a notebook:
    from matplotlib import pyplot as plt
    plt.ion()
#+END_EXAMPLE

=momi= uses [[https://matplotlib.org/][matplotlib]] for plotting. Users
can make calls to =matplotlib= to adjust various aspects of the
plotting.

** Reading and simulating data
   :PROPERTIES:
   :CUSTOM_ID: reading-and-simulating-data
   :END:

In this section we demonstrate how to read in data from a VCF file.

Before we can read in a dataset we first need to create one. So we start
by demonstrating how to simulate a VCF from our demography, then we read
it in.

*** Simulating data
    :PROPERTIES:
    :CUSTOM_ID: simulating-data
    :END:

    We simulate a dataset of diploid individuals,
    with 20 "chromosomes" of length 50Kb, with a recombination
    rate of 1.25e-8. =momi= uses =msprime= under the hood to do this.
    
#+BEGIN_SRC ipython
  recoms_per_gen = 1.25e-8
  bases_per_locus = int(5e5)
  n_loci = 20
  ploidy = 2

  # n_alleles per population (n_individuals = n_alleles / ploidy)
  sampled_n_dict = {"NEA":2, "YRI":4, "CHB":4}

  # create data directory if it doesn't exist
  import os
  os.makedirs("data", exist_ok=True)

  # simulate 20 "chromosomes", saving each in a separate vcf file
  for chrom in range(n_loci):
        model.simulate_vcf(
              f"data/{chrom}",
              recoms_per_gen=recoms_per_gen,
              length=bases_per_locus,
              chrom_name="chr{}".format(chrom),
              ploidy=ploidy,
              random_seed=1234+chrom,
              sampled_n_dict=sampled_n_dict,
              force=True)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
:END:

*** Read in data from vcf

    First we need to create a mapping from individuals to populations.
    We save this mapping to a text file whose first column
    is for individuals and second column is for populations.
    
#+BEGIN_SRC ipython
  # a dict mapping samples to populations
  ind2pop = {}
  for pop, n in sampled_n_dict.items():
      for i in range(int(n / ploidy)):
          # in the vcf, samples are named like YRI_0, YRI_1, CHB_0, etc
          ind2pop["{}_{}".format(pop, i)] = pop

  with open("data/ind2pop.txt", "w") as f:
      for i, p in ind2pop.items():
          print(i, p, sep="\t", file=f)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[38]:
:END:

**** Command line API

From the command line, use python -m momi.vcf_allele_counts to
compute allele counts for each VCF.

The --bed flag is used to read in a BED file specifying which
regions of the VCF to read. It is also used to determine
the size of the data in bases. It is very important not
to use the same BED file for chromosomes stored in separate
VCF files, otherwise regions will be double-counted when
computing the length of the data!

By default ancestral alleles are read from the INFO AA field
(SNPs missing this field are skipped) but this behavior can be
changed via the flags --no_aa or --outgroup.

Use the --help flag to see more command line options,
and the Python API for =momi.SnpAlleleCounts.read_vcf=
for more details on the parameters.

Below we read in each VCF file, compute the allele counts,
and store the result in data/$chrom.snpAlleleCounts.gz

#+BEGIN_SRC ipython
  %%sh
  for chrom in `seq 0 19`;
  do
      echo $chrom
      python -m momi.vcf_allele_counts \
             data/$chrom.vcf.gz data/ind2pop.txt \
             data/$chrom.snpAlleleCounts.gz \
             --bed data/$chrom.bed
  done
#+END_SRC

Use python -m momi.extract_sfs to combine the allele counts across
multiple files and extract the SFS. It also splits the SFS
into a number of equally sized blocks (100 in the example below),
for jackknifing and bootstrapping later.

#+BEGIN_SRC ipython
  %%sh
  python -m momi.extract_vcf data/sfs.gz 100 data/*.snpAlleleCounts.gz
#+END_SRC
    
From within Python, read in the resulting SFS with
=momi.site_freq_spectrum.load=.


#+BEGIN_SRC ipython
  with open("data/sfs.gz") as f:
      sfs = momi.site_freq_spectrum.load(f)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[12]:
: True
:END:

** Inference

   In this section we will infer a demography for the data we simulated.
   We will start by fitting a sub-demography on CHB and YRI, and then
   iteratively build on this model, by adding the NEA population and also
   additional parameters and events.

*** An initial model for YRI and CHB

   We start by creating a =DemographicModel= object as before:

#+BEGIN_SRC ipython
  model2 = momi.DemographicModel(N_e=1.2e4, gen_time=29,
                                 muts_per_gen=1.25e-8)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[13]:
:END:

   Note the =muts_per_gen= keyword is optional, but helpful:
   if the mutation rate is provided then =momi= can use it to fit the
   total number of SNPs in the dataset.
   Otherwise =momi= will not be able to use
   this information, potentially losing some power.

We provide the model with a dataset by calling =set_data()=:

#+BEGIN_SRC ipython
  model2.set_data(sfs)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[14]:
:END:

Parameters can be added to the model via the methods
=add_size_param()=, =add_growth_param()=, =add_time_param()=,
and =add_pulse_param()=.

#+BEGIN_SRC ipython
    model2.add_size_param("n_chb")
    model2.add_growth_param("g_chb", 0, lower=-1e-3, upper=1e-3)
    model2.add_time_param("t_chb_yri", lower=1e4)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[15]:
:END:

Above we define parameters for the CHB size, the CHB growth rate,
and the CHB-YRI split time.
The size and split time were initialized with random values, while
we initialized the growth rate as 0.
We also provide some optional lower and upper bounds to these parameters.

Demographic events can be added similarly as before. 
Parameters are specified by name (string), 
while constants are specified as numbers (float).

#+BEGIN_SRC ipython
    model2.add_leaf("CHB", N="n_chb", g="g_chb")
    model2.add_leaf("YRI")
    model2.set_size("CHB", t=1e4, g=0)
    model2.move_lineages("CHB", "YRI", t="t_chb_yri")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[16]:
:END:

Use the method =optimize()= to search for the MLE.
It accepts parameters similar to =scipy.optimize.minimize=
(it is just a thin wrapper around that function).

#+BEGIN_SRC ipython :async t
    model2.optimize(method="TNC")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[17]:
#+BEGIN_EXAMPLE
  fun: 0.0013517432433607492
  jac: array([ 8.13244554e-05, -9.27463390e-01, -1.56743636e-08])
  kl_divergence: 0.0013517432433607492
  log_likelihood: -33215.69100703143
  message: 'Converged (|f_n-f_(n-1)| ~= 0)'
  nfev: 33
  nit: 13
  parameters: ParamsDict({'n_chb': 86175.04788454846, 'g_chb': 0.0004638996288466491, 't_chb_yri': 110975.47379809801})
  status: 1
  success: True
  x: array([1.13641359e+01, 4.63899629e-04, 1.00975474e+05])
#+END_EXAMPLE
:END:

The default optimization method is "TNC" (truncated Newton conjugate).
This is very accurate but can be slow for large models; for large models,
method="L-BFGS-B" is a good choice.

We can print the inferred parameter values with =get_params()=:

#+BEGIN_SRC ipython
    model2.get_params()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[18]:
#+BEGIN_EXAMPLE
  ParamsDict([('n_chb', 86175.04788454846),
  ('g_chb', 0.0004638996288466491),
  ('t_chb_yri', 110975.47379809801)])
#+END_EXAMPLE
:END:

and we can plot the inferred demography as before: 

#+BEGIN_SRC ipython
    # plot the model
    fig = momi.DemographyPlot(model2, ["YRI", "CHB"],
                              figsize=(6,8), linthreshy=1e5,
                              major_yticks=yticks,
                              pulse_bounds=(0,.25))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[19]:
[[file:./obipy-resources/1676faY.png]]
:END:

*** Adding NEA to the existing model

    Now we add in the NEA population, along with a parameter for its split time
    =t_anc=. We use the keyword =lower_constraints= to require that =t_anc > t_chb_yri=.

#+BEGIN_SRC ipython
    model2.add_leaf("NEA", t=5e4)
    model2.add_time_param("t_anc", lower=5e4, lower_constraints=["t_chb_yri"])
    model2.move_lineages("YRI", "NEA", t="t_anc")

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[20]:
:END:

We search for the new MLE and plot the inferred demography:

#+BEGIN_SRC ipython :async t
  model2.optimize()

  fig = momi.DemographyPlot(
      model2, ["YRI", "CHB", "NEA"],
      figsize=(6,8), linthreshy=1e5,
      major_yticks=yticks)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[21]:
[[file:./obipy-resources/1676ske.png]]
:END:

*** Evaluating model fit (ABBA-BABA, sequence divergence)

The =SfsModelFitStats= class can be used to compute various 
statistics of the SFS and see how well they fit the model.

One such statistic is the =f4()= or "ABBA-BABA" statistic for
detecting introgression. In the absence of admixture it should be 0,
but it is significantly negative for our dataset:

#+BEGIN_SRC ipython :results raw drawer output
  model_fit_stats = momi.SfsModelFitStats(model2)

  print("Computing f4(YRI, CHB, NEA, AncestralAllele)")
  f4 = model_fit_stats.f4("YRI", "CHB", "NEA", None)

  print("Expected = {}".format(f4.expected))
  print("Observed = {}".format(f4.observed))
  print("SD = {}".format(f4.sd))
  print("Z(Expected-Observed) = {}".format(f4.z_score))
#+END_SRC

#+RESULTS:
:RESULTS:
Computing f4(YRI, CHB, NEA, AncestralAllele)
Expected = 0.0
Observed = -0.005661024702653249
SD = 0.002716661599502775
Z(Expected-Observed) = -2.0838166607461797
:END:

Other statistics such as =f2()= and =f3()= are also available
for evaluating model fit.
The method =all_pairs_ibs()= evaluates the probability that 
two random alleles are the same, for every pair of populations:

#+BEGIN_SRC ipython
  model_fit_stats.all_pairs_ibs()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[22]:
#+BEGIN_EXAMPLE
  Pop1 Pop2  Expected  Observed         Z
  0  CHB  NEA  0.545142  0.552024  1.032563
  1  YRI  YRI  0.715829  0.718847  0.798355
  2  NEA  YRI  0.545142  0.541331 -0.786961
  3  CHB  YRI  0.671057  0.674834  0.613838
  4  NEA  NEA  0.715829  0.710842 -0.599368
  5  CHB  CHB  0.944302  0.946577  0.363225
#+END_EXAMPLE
[[file:./obipy-resources/1676FGM.png]]
:END:

In addition, the method =SfsModelFitStats.tensor_prod= can be used to
compute goodness-of-fit for very general statistics of the
SFS; see the API for details.

Note the =SfsModelFitStats= class above has some limitations.
First, it computes goodness-of-fit for the SFS without any missing data --
all entries with missing samples are removed. For datasets
with many individuals and pervasive missingness, this can result
in most or all of the data being removed.

In such cases you can specify to use the SFS restricted
to a smaller number of samples -- then SNPs with at least
that many of non-missing individuals will be used.
For example,

#+BEGIN_SRC ipython
  model_fit_stats = momi.SfsModelFitStats(
      model2, {"YRI": 2, "CHB": 2, "NEA": 2})
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[23]:
:END:

will compute statistics for the SFS 
restricted to 2 samples per population.

The second limitation of =SfsModelFitStats= is that it
ignores the mutation rate -- it only fits the SFS normalized
to be a probability distribution.
To evaluate the total number of mutations in the data,
e.g. to fit the mutation rate, you can use the method
=DemographicModel.fit_within_pop_diversity()=,
which computes the within-population nucleotide
diversity, i.e. the heterozygosity of a random
individual from that population:

    #+BEGIN_SRC ipython :results raw drawer
      model2.fit_within_pop_diversity()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[24]:
    #+BEGIN_EXAMPLE
      Pop    EstMutRate   JackknifeSD  JackknifeZscore
      0  CHB  1.241305e-08  1.454909e-09        -0.059764
      1  YRI  1.280417e-08  1.720568e-10         1.767827
      2  NEA  1.316875e-08  3.791194e-10         1.763956
    #+END_EXAMPLE
    :END:

This method returns a dataframe giving estimates for the mutation rate.
Note that there is an estimate for each population -- these estimates
are non-independent estimates for the same value, just computed
in different ways (by computing the expected to observed heterozygosity
for each population separately). These estimates
account for missingness in the data -- it is fine to use it
on datasets with large amounts of missingness.

Since we initialized our model with =muts_per_gen=1.25e-8=,
the method also returns a Z-value for the residuals of the estimated
mutation rates.

*** Build a new model adding NEA->CHB

Now we create a new =DemographicModel=,
by copying the previous model and adding a NEA->CHB
migration arrow.

#+BEGIN_SRC ipython
  add_pulse_model = model2.copy()
  add_pulse_model.add_pulse_param("p_pulse", upper=.25)
  add_pulse_model.add_time_param(
      "t_pulse", upper_constraints=["t_chb_yri"])

  add_pulse_model.move_lineages(
      "CHB", "GhostNea", t="t_pulse", p="p_pulse")

  add_pulse_model.add_time_param(
      "t_ghost", lower=5e4,
      lower_constraints=["t_pulse"], upper_constraints=["t_anc"])
  add_pulse_model.move_lineages(
      "GhostNea", "NEA", t="t_ghost")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[22]:
:END:

It turns out this model has local optima so we will
fit a few independent runs starting from different positions.
=DemographicModel.set_params()= can be used to either
set a parameter explicitly, or to choose a random value:

#+BEGIN_SRC ipython :async t
  results = []
  n_runs = 3
  for i in range(n_runs):
      print(f"Run {i+1} out of {n_runs}...")
      add_pulse_model.set_params(
          # parameters inherited from model2 are set to their previous values
          model2.get_params(),
          # other parmaeters are set to random initial values
          randomize=True)

      results.append(add_pulse_model.optimize())

  # sort results according to log likelihood, pick the best one
  best_result = sorted(results, key=lambda r: r.log_likelihood)[0]

  add_pulse_model.set_params(best_result.parameters)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[26]:
:END:

#+BEGIN_SRC ipython
  # plot the model
  fig = momi.DemographyPlot(
      add_pulse_model, ["YRI", "CHB", "GhostNea", "NEA"],
      linthreshy=1e5, figsize=(6,8),
      major_yticks=yticks)
  fig.draw_N_legend(loc="upper left")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[30]:
: <matplotlib.legend.Legend at 0x7f99a2fdbf28>
[[file:./obipy-resources/1676SXG.png]]
:END:

*** Bootstrap confidence intervals

To generate confidence intervals, we can resample blocks of the SFS,
refit the model, and examine the quantiles of the re-inferred parameters.

Below we do this for a very small number of bootstraps and a simplified
fitting procedure. In practice you would want to generate hundreds of bootstraps
on a cluster computer.

#+BEGIN_SRC ipython :async t
  n_bootstraps = 5
  # make copies of the original models
  submodel_copy = model2.copy()
  add_pulse_copy = add_pulse_model.copy()

  bootstrap_results = []
  for i in range(n_bootstraps):
      print(f"Fitting {i+1}-th bootstrap out of {n_bootstraps}")

      # resample the data
      resampled_sfs = sfs.resample()
      # tell models to use the new dataset
      submodel_copy.set_data(resampled_sfs)
      add_pulse_copy.set_data(resampled_sfs)

      # choose new random parameters for submodel, optimize
      submodel_copy.set_params(randomize=True)
      submodel_copy.optimize()
      # initialize parameters from submodel, randomizing the new parameters
      add_pulse_copy.set_params(submodel_copy.get_params(),
                                randomize=True)
      add_pulse_copy.optimize()

      bootstrap_results.append(add_pulse_copy.get_params())
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[31]:
:END:

We can visualize the bootstrap results by overlaying them onto a single plot.

#+BEGIN_SRC ipython
  # make canvas, but delay plotting the demography (draw=False)
  fig = momi.DemographyPlot(
      add_pulse_model, ["YRI", "CHB", "GhostNea", "NEA"],
      linthreshy=1e5, figsize=(6,8),
      major_yticks=yticks,
      draw=False)

  # plot bootstraps onto the canvas in transparency
  for params in bootstrap_results:
      fig.add_bootstrap(
          params,
          # alpha=0: totally transparent. alpha=1: totally opaque
          alpha=1./n_bootstraps)

  # now draw the inferred demography on top of the bootstraps
  fig.draw()
  fig.draw_N_legend(loc="upper left")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[35]:
: <matplotlib.legend.Legend at 0x7f99a3e03c18>
[[file:./obipy-resources/1676GAf.png]]
:END:

** Other features
*** Stochastic gradient descent

For large models, it can be useful to perform stochastic optimization:
instead of computing the full likelihood at every step,
we use a random subset of SNPs at each step to estimate
the likelihood gradient. This is especially useful for
rapidly searching for a reasonable starting point, from which
full optimization can be performed.

=DemographicModel.stochastic_optimize()= 
implements stochastic optimization with the ADAM
method for using second order information.
Setting svrg=n makes the optimizer use the full likelihood
every n steps which can lead to better convergence (see the
Stochastic Variance Reduced Gradient algorithm).

The cell below performs 10 steps of stochastic optimization,
using 1000 random SNPs per step, and computing the full likelihood
every 3 iterations.

#+BEGIN_SRC ipython :async t
  add_pulse_copy.stochastic_optimize(
      snps_per_minibatch=1000, num_iters=10, svrg_epoch=3)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[37]:
#+BEGIN_EXAMPLE
  fun: 3.675319582274675
  jac: array([ 2.45902394e-06, -2.47377345e-03, -1.40661715e-07, -1.19779060e-07,
  2.84491207e-02,  1.88199504e-05, -5.77398319e-26])
  message: 'Maximum number of iterations reached'
  nit: 9
  success: False
  x: array([ 1.64747455e+01, -1.00000000e-03,  7.16960160e+04,  3.83693192e+05,
  -3.57757754e+00, -1.49613392e+00, -5.21281622e+01])
#+END_EXAMPLE
:END:
