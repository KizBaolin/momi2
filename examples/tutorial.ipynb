{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import momi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **`help()`** function to view documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package momi:\n",
      "\n",
      "NAME\n",
      "    momi\n",
      "\n",
      "DESCRIPTION\n",
      "    momi (MOran Models for Inference) is a python package for computing the site frequency spectrum,\n",
      "    a summary statistic commonly used in population genetics, and using it to infer demographic history.\n",
      "    \n",
      "    Please refer to examples/tutorial.ipynb for usage & introduction.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    compute_sfs\n",
      "    confidence_region\n",
      "    convolution\n",
      "    data_structure\n",
      "    demography\n",
      "    likelihood\n",
      "    math_functions\n",
      "    moran_model\n",
      "    optimizers\n",
      "    parse_ms\n",
      "    size_history\n",
      "    tensor\n",
      "    util\n",
      "\n",
      "FILE\n",
      "    /home/jack/anaconda3/envs/momi2/lib/python3.5/site-packages/momi/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(momi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating demographies\n",
    "\n",
    "momi uses a syntax based on the program ms by Richard Hudson.\n",
    "A demography is specified as a sequence of events.\n",
    "Time is measured going backwards from the present (t=0) to the past (t>0).\n",
    "\n",
    "There are 4 kinds of events:\n",
    "* **('-en', t, i, N)**\n",
    "    * At time t, population i has its scaled population size set to N, and growth rate set to 0.\n",
    "* **('-eg', t, i, g)**\n",
    "    * At time t, population i has exponential growth rate g (so for s>t, $N(s) = N(t) e^{(s-t)  g}$)\n",
    "* **('-ej', t, i, j)**\n",
    "    * At time t, all lineages in population i move into j.\n",
    "* **('-ep', t, i, j, p_ij)**\n",
    "    * At time t, each lineage in i moves into j with probability p_ij.\n",
    "\n",
    "Note **-en,-eg,-ej** are flags from ms, while **-ep** replaces the flag **-es** in ms.\n",
    "By default, all parameters are scaled as in ms, but this can be adjusted.\n",
    "\n",
    "See **`help(momi.Demography)`** or **`help(momi.Demography.__init__)`** for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example demography\n",
    "\n",
    "Here we consider a concrete example. More examples can be found at [example_demographies.ipynb](files/example_demographies.ipynb).\n",
    "\n",
    "Unlike ms, populations can be labeled by arbitrary strings.  \n",
    "Here we label the sampled populations as **'chb'** and **'yri'**.  \n",
    "The demography also involves admixture with a third population, **'nea'**.\n",
    "\n",
    "By default, the parameters are scaled like in **ms**. The population sizes are assumed to be rescaled by a \"reference\" size N_ref (e.g. 10,000), and time is scaled so there are 4*N_ref generations per unit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the list of events\n",
    "events = [('-en', 0., 'chb', 10.),         # at present (t=0), 'chb' has diploid population size 10 * N_ref\n",
    "          ('-eg', 0, 'chb' , 6.),          # at present (t=0), 'chb' growing at rate 6\n",
    "          ('-ep', .25, 'chb', 'nea', .03), # at t=.25, 'chb' has a bit of admixture from 'nea'\n",
    "          ('-ej', .5, 'chb', 'yri'),       # at t=.5, 'chb' joins onto 'yri' \n",
    "          ('-ej', 1.5, 'yri', 'nea'),      # at t=1.5, 'yri' joins onto 'nea'\n",
    "          ]\n",
    "\n",
    "# construct the Demography object, sampling 14 alleles from 'yri' and 10 alleles from 'chb'\n",
    "demo = momi.make_demography(events, sampled_pops=('yri','chb'), sampled_n=(14,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coalescent statistics\n",
    "\n",
    "`momi` can compute coalescent statistics such as the TMRCA (time to most recent common ancestor) and total branch length of the genealogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected TMRCA of all samples: \t 1.4192051765290357\n",
      "Expected TMRCA of chb samples: \t 1.2537429545633438\n",
      "Expected total branch length: \t 7.93963743415\n"
     ]
    }
   ],
   "source": [
    "eTmrca = momi.expected_tmrca(demo)\n",
    "print(\"Expected TMRCA of all samples:\", \"\\t\", eTmrca)\n",
    "\n",
    "eTmrca_chb = momi.expected_deme_tmrca(demo, 'chb')\n",
    "print(\"Expected TMRCA of chb samples:\", \"\\t\", eTmrca_chb)\n",
    "\n",
    "eL = momi.expected_total_branch_len(demo)\n",
    "print (\"Expected total branch length:\", \"\\t\", eL)\n",
    "\n",
    "# See help(momi.expected_tmrca), etc. for more details.\n",
    "# Advanced users can use momi.expected_sfs_tensor_prod()\n",
    "# to compute these and many other summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Sample Frequency Spectrum (SFS)\n",
    "\n",
    "The expected SFS for configuration $((a_0,d_0),(a_1,d_1),...)$ is the expected number of SNPs with $a_0$ ancestral and $d_0$ derived alleles in population 0, $a_1$ ancestral and $d_1$ derived alleles in population 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.6309565   0.95647102  0.0048574   0.06108049  0.04371188  0.00323204]\n"
     ]
    }
   ],
   "source": [
    "# a list of configs (index0 == yri, index1 == chb)\n",
    "configs = [( (14,0), (9,1), ), # 1 derived allele in chb\n",
    "           ( (13,1), (10,0), ), # 1 derived allele in yri \n",
    "           ( (11,3), (9,1),) , # 3 derived in yri, 1 derived in chb\n",
    "           ( (14,0), (0,10), ), # 0 derived in yri, all derived in chb\n",
    "           ( (2,12), (10,0), ), # 12 derived in yri, 0 derived in chb \n",
    "           ( (2,12), (2,8), ), # 12 derived in yri, 8 derived in chb\n",
    "          ]\n",
    "\n",
    "configs = momi.config_array((\"yri\",\"chb\"), configs)\n",
    "\n",
    "# the SFS entries corresponding to each config in configs\n",
    "eSFS = momi.expected_sfs(demo, configs, mut_rate=1.0)\n",
    "print (eSFS)\n",
    "\n",
    "# See help(momi.expected_sfs) for more options (e.g. folded SFS, sampling error, normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segregating sites\n",
    "\n",
    "A dataset of segregating sites has been stored in [tutorial_data.txt](tutorial_data.txt). The file is organized as follows:\n",
    "* First line gives the population labels.\n",
    "* Each locus starts with a line \"**//**\".\n",
    "* Subsequent lines correspond to segregating sites, and give the **ancestral,derived** allele counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yri\tchb\r\n",
      "\r\n",
      "//\r\n",
      "\r\n",
      "14,0\t6,4\r\n",
      "14,0\t9,1\r\n",
      "13,1\t10,0\r\n",
      "14,0\t9,1\r\n",
      "1,13\t10,0\r\n",
      "14,0\t0,10\r\n"
     ]
    }
   ],
   "source": [
    "# print first few lines of tutorial_data.txt\n",
    "!head tutorial_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`momi.read_seg_sites`** reads the file, and returns a **`momi.SegSites`** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the file with momi\n",
    "data_filename = \"tutorial_data.txt\"\n",
    "with open(data_filename,'r') as f:\n",
    "    seg_sites = momi.read_seg_sites(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting **momi.SegSites** object is essentially a list of lists; **`momi.SegSites[i][j]`** is the configuration of the $j$th SNP on locus $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loci: 20\n",
      "Number of SNPs on locus 0: 3411\n",
      "Configuration of SNP 0 on locus 0:\n",
      " [[14  0]\n",
      " [ 6  4]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of loci:\", len(seg_sites))\n",
    "print (\"Number of SNPs on locus 0:\", len(seg_sites[0]))\n",
    "print (\"Configuration of SNP 0 on locus 0:\\n\", seg_sites[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**momi.SegSites** also has a few additional attributes, such as the population labels and sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population labels: ('yri', 'chb')\n",
      "Sample sizes: [14 10]\n"
     ]
    }
   ],
   "source": [
    "print (\"Population labels:\", seg_sites.sampled_pops)\n",
    "print (\"Sample sizes:\", seg_sites.sampled_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **momi.SegSites.sfs** attribute stores a **momi.Sfs** object.\n",
    "\n",
    "**momi.Sfs.configs** stores a **momi.ConfigArray** object (essentially a list of the unique configs),\n",
    "and **momi.Sfs.freq_counts** gives a sparse matrix whose `[i,j]` entry is the frequency (count) of\n",
    "**`configs[j]`** on locus $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique configs: 162\n",
      "First five configs:\n",
      " [[[14  0]\n",
      "  [ 6  4]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 9  1]]\n",
      "\n",
      " [[13  1]\n",
      "  [10  0]]\n",
      "\n",
      " [[ 1 13]\n",
      "  [10  0]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 0 10]]]\n",
      "Frequency of config 3 on locus 6: 16.0\n",
      "Dimensions of SFS frequency matrix: (162, 20)\n"
     ]
    }
   ],
   "source": [
    "## the sfs object\n",
    "sfs = seg_sites.sfs\n",
    "configs = sfs.configs\n",
    "\n",
    "print (\"Number of unique configs:\", len(configs))\n",
    "print (\"First five configs:\\n\", configs[:5])\n",
    "print (\"Frequency of config 3 on locus 6:\", sfs.freqs_matrix[3,6])\n",
    "print (\"Dimensions of SFS frequency matrix:\", sfs.freqs_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to reading in from file, the **configs**, **sfs**, and **seg_sites** objects above can be directly created with the functions **momi.config_array()**, **momi.site_freq_spectrum()**, and **momi.seg_site_configs()** respectively. See their **help()** for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "To start, we need to define a function mapping **parameters** to a **Demography**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np ## thinly wrapped version of numpy for auto-differentiation\n",
    "\n",
    "def demo_func(N_chb_bottom, N_chb_top, pulse_t, pulse_p, ej_chb, ej_yri):   \n",
    "    # use autograd.numpy for math functions (e.g. exp, logarithm)\n",
    "    # This will allow us to take derivatives later    \n",
    "    \n",
    "    ej_chb = pulse_t + ej_chb\n",
    "    ej_yri = ej_chb + ej_yri\n",
    "    \n",
    "    N_chb_top = N_chb_top\n",
    "    N_chb_bottom = N_chb_bottom\n",
    "    \n",
    "    G_chb = -np.log(N_chb_top / N_chb_bottom) / ej_chb\n",
    "    \n",
    "    events = [('-en', 0., 'chb', N_chb_bottom),\n",
    "              ('-eg', 0, 'chb' , G_chb),\n",
    "              ('-ep', pulse_t, 'chb', 'nea', pulse_p),\n",
    "              ('-ej', ej_chb, 'chb', 'yri'),\n",
    "              ('-ej', ej_yri, 'yri', 'nea'),\n",
    "              ]\n",
    "\n",
    "    return momi.make_demography(events, ('yri','chb'), (14,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `autograd.numpy` as a drop-in replacement for `numpy` that allows for **automatic differentiation**, which is useful for searching for the maximum likelihood.\n",
    "\n",
    "This also let us compute gradients of other functions that call `demo_func()`. For example, we can compute the gradient of the TMRCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [10.0, 0.1, 0.25, 0.03, 0.25, 1.0]\n",
      "Expected TMRCA: 1.3127771621846782\n",
      "Gradient:\n",
      "[0.0045248167517458089, 0.55838496083537059, 0.28449869706692221, 3.9671067152725463, 0.83175299796269575, 0.13916390098743403]\n"
     ]
    }
   ],
   "source": [
    "import autograd\n",
    "\n",
    "# function mapping vector of parameters to the TMRCA of the corresponding demography\n",
    "def tmrca_func(params):\n",
    "    # equivalent to demo_func(params[0], params[1], params[2], ...)\n",
    "    demo = demo_func(*params)\n",
    "    # return expected TMRCA\n",
    "    return momi.expected_tmrca(demo)\n",
    "\n",
    "# use autograd.grad() to obtain the gradient function\n",
    "tmrca_grad = autograd.grad(tmrca_func)\n",
    "\n",
    "x = [10., .1, .25, .03, .25, 1.]\n",
    "print (\"Parameters:\", x)\n",
    "print (\"Expected TMRCA:\", tmrca_func(x))\n",
    "print (\"Gradient:\")\n",
    "print (tmrca_grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## However, autograd has some important limitations!\n",
    "\n",
    "If you are unsure whether you are using autograd correctly, `momi` also lets you use 0th-order methods (such as Nelder-Mead)\n",
    "or numerical gradients via finite-differences.\n",
    "\n",
    "### More details about correctly using `autograd`\n",
    "\n",
    "Documentation for autograd can be found at https://github.com/HIPS/autograd/\n",
    "\n",
    "Some do's and don'ts for autograd (from the tutorial):\n",
    "* Do use\n",
    "    * Arithmetic operations `+,-,*,/,**`\n",
    "    * Most of `numpy`'s functions\n",
    "    * Most `numpy.ndarray` methods\n",
    "    * Some `scipy` functions\n",
    "    * Indexing and slicing of arrays `x = A[3, :, 2:4]`\n",
    "    * Explicit array creation from lists `A = np.array([x, y])`\n",
    "* Don't use\n",
    "    * Assignment to arrays `A[0,0] = x`\n",
    "    * Implicit casting of lists to arrays `A = numpy.sum([x, y])`, use `A = numpy.sum(np.array([x, y]))` instead.\n",
    "    * `A.dot(B)` notation (use `numpy.dot(A, B)` instead)\n",
    "    * In-place operations (such as `a += b`, use `a = a + b` instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's try inferring some parameters from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_params = [10., .1, .25, .03, .25, 1.]\n",
    "true_demo = demo_func(*true_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set **`ms_path = \"/path/to/ms\"`** here to generate a new dataset with ms. Otherwise, use the dataset stored in `tutorial_data.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ms_path provided, using SFS previously stored in tutorial_data.txt.\n"
     ]
    }
   ],
   "source": [
    "ms_path = None\n",
    "#import os; ms_path = os.environ[\"MS_PATH\"]\n",
    "    \n",
    "if ms_path is not None:\n",
    "    print (\"Generating the dataset with ms\")\n",
    "    \n",
    "    n_loci = 20\n",
    "    kb_per_locus = 500\n",
    "    mut_rate_per_kb, recom_rate_per_kb = 1.,1.\n",
    "    \n",
    "    seg_sites = momi.simulate_ms(ms_path, true_demo, \n",
    "                                 n_loci, kb_per_locus * mut_rate_per_kb,\n",
    "                                 #seeds = (8444303657, 6969571397, 652167805),\n",
    "                                 additional_ms_params=\"-r %f %d\" % (kb_per_locus*recom_rate_per_kb,\n",
    "                                                                    int(kb_per_locus * 1000)))\n",
    "    sfs = seg_sites.sfs\n",
    "    \n",
    "    #with open(data_filename,'w') as f:\n",
    "    #    momi.write_seg_sites(f, seg_sites)\n",
    "else:\n",
    "    print (\"No ms_path provided, using SFS previously stored in %s.\" % data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define (lower,upper) bounds on the parameter space\n",
    "bounds = [(.01, 100.),\n",
    "          (.01, 100.),\n",
    "          (.01,5.),\n",
    "          (1e-100,.25),\n",
    "          (.01,5.),\n",
    "          (.01,5.)]\n",
    "\n",
    "# pick a random start value for the parameter search\n",
    "import random\n",
    "lower_bounds, upper_bounds = [l for l,u in bounds], [u for l,u in bounds]\n",
    "start_params = [random.triangular(lower, upper, mode)\n",
    "                for lower, upper, mode in zip(lower_bounds, upper_bounds, [1, 1, 1, .1, 1,1,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the MCLE (max composite likelihood estimate) by first creating a\n",
    "likelihood surface with **momi.SfsLikelihoodSurface()**, and then calling its method **find_mle()**.\n",
    "\n",
    "`find_mle()` is basically a wrapper around the generic `scipy.optimize.minimize()` minimizer,\n",
    "so you can use any of the methods there (e.g. Nelder-Mead, L-BFGS-B, tnc, etc.). The current default is \"tnc\".\n",
    "\n",
    "By default, `find_mle()` assumes `demo_func` is differentiable with `autograd`, and uses the gradient in a hill-climbing algorithm.\n",
    "* If you don't want to use `autograd`, you can disable the gradient (i.e. jacobian) with:\n",
    "    * `surface.find_mle(..., jac=False, ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surface = momi.SfsLikelihoodSurface(seg_sites, demo_func)\n",
    "with open('tutorial.log','w') as log_file: # store intermediate output in file\n",
    "    mcle_search_result = surface.find_mle(start_params, bounds = bounds, options={'maxiter':1000}, out=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results:\n",
      "     fun: 0.0022467365501257931\n",
      "     jac: array([  6.47273250e-08,   2.74144908e-05,   2.51113179e-06,\n",
      "         3.86109518e-05,   1.23024927e-06,   9.20412879e-07])\n",
      " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
      "    nfev: 91\n",
      "     nit: 19\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([ 9.88111201,  0.10180005,  0.25353253,  0.03316861,  0.24672514,\n",
      "        1.06539863])\n"
     ]
    }
   ],
   "source": [
    "# print info such as whether search succeeded, number function/gradient evaluations, etc\n",
    "print (\"Search results:\")\n",
    "print (mcle_search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated params:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9.88111201,  0.10180005,  0.25353253,  0.03316861,  0.24672514,\n",
       "        1.06539863])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_params = mcle_search_result.x\n",
    "print (\"Estimated params:\")\n",
    "est_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence regions and Hypothesis testing\n",
    "\n",
    "`momi` uses the **Limit of Experiments** to compute approximate confidence regions and hypothesis tests.  \n",
    "A good reference is *Asymptotic Statistics* by **A.W. van der Vaart**.  \n",
    "(Some readers may be familiar with the **Godambe Information**, which uses a special case of this theory.)\n",
    "\n",
    "If certain regularity conditions hold, then the confidence regions will have the correct coverage\n",
    "properties as the data $\\to \\infty$.  \n",
    "In practice it is important to check the coverage via simulation, because:\n",
    "* It is not well understood when the regularity conditions hold in demographic inference.\n",
    "* Even if the regularity conditions hold, the coverage will be inaccurate if there is not enough data.\n",
    "\n",
    "`momi` can construct the asymptotic confidence regions for 2 limiting regimes:\n",
    "* **\"long\"**: fixed number of loci, whose length $\\to \\infty$\n",
    "    * Treats the segregating sites along each locus as a time series\n",
    "    * The loci should be independent (unlinked), but don't have to be identically distributed\n",
    "* **\"many\"**: many short loci, with the number of loci ` $\\to \\infty$\n",
    "    * The loci should be independent, and roughly identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a confidence region around the estimated parameters\n",
    "confidence_region = momi.ConfidenceRegion(est_params, demo_func, seg_sites, regime=\"long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value at the truth, using likelihood ratio test.  \n",
    "Asymptotically, the log-likelihood ratio is a transformation of a Gaussian.  \n",
    "We use **`ConfidenceRegion.test()`** to compute an approximate p-value, by an inexpensive simulation of 1000 Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value at true params: 0.419\n"
     ]
    }
   ],
   "source": [
    "print (\"p-value at true params:\", confidence_region.test(true_params, sims=int(1e3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ConfidenceRegion.test()`** can also compute the p-values at a list of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.057,  0.91 ,  0.079])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_region.test([est_params * .95, est_params * .98, est_params * 1.05], sims=int(1e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ConfidenceRegion.test()`** can also construct tests and intervals based on the Wald-statistic.  \n",
    "This does not require simulation, but typically the Wald statistic is less powerful and converges more slowly than the likelihood ratio.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value at true params (Wald): 0.7842564537359138\n"
     ]
    }
   ],
   "source": [
    "print (\"p-value at true params (Wald):\", confidence_region.test(true_params, test_type=\"wald\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of Wald is the ease of computing marginal intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% marginal confidence intervals\n",
      "[[  8.88034914  10.88187488]\n",
      " [  0.08845396   0.11514615]\n",
      " [  0.22894386   0.27812121]\n",
      " [  0.02191629   0.04442094]\n",
      " [  0.21919494   0.27425533]\n",
      " [  0.775189     1.35560826]]\n"
     ]
    }
   ],
   "source": [
    "print (\"95% marginal confidence intervals\")\n",
    "print (confidence_region.wald_intervals(lower=.025,upper=.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex hypothesis tests\n",
    "\n",
    "**`ConfidenceRegion.test()`** can also perform more complicated tests for nested models.  \n",
    "We consider testing that the pulse probability is 0.  \n",
    "We will also fix the pulse time and \"nea\" join time; otherwise the demography is not identifiable, which violates a crucial regularity condition.  \n",
    "(If the pulse probability is 0, then the pulse time, and the \"nea\" join on time, have no affect on the likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the MLE in the null model space\n",
    "start0, bound0 = list(est_params), list(bounds)\n",
    "start0[2] = bound0[2] = est_params[2] # fix the pulse time\n",
    "start0[5] = bound0[5] = est_params[5] # fix the 'nea' join time\n",
    "start0[3] = bound0[3] = 1e-100 # set the pulse probability to 0\n",
    "\n",
    "constrained_opt_result = surface.find_mle(start0, bounds = bound0, options={'maxiter':500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE of null model:\n",
      "[  8.48932856e+000   1.92703092e-001   2.53532533e-001   1.00000000e-100\n",
      "   2.85824595e-001   1.06539863e+000]\n"
     ]
    }
   ],
   "source": [
    "print (\"MLE of null model:\")\n",
    "est0 = constrained_opt_result.x\n",
    "print (est0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate p-value of no migration: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Null model:\n",
    "# parameters 0,1,4 unconstrained (\"None\")\n",
    "# parameters 2,3,5 are fixed (\"0\")\n",
    "null_cone = (None,None,0,0,None,0)\n",
    "\n",
    "# Alternative model:\n",
    "# parameters 0,1,4, unconstrained (\"None\")\n",
    "# parameters 2,5 fixed (\"0\")\n",
    "# parameter 3 bounded on the left (\"1\". use \"-1\" if bounded on the right)\n",
    "alt_cone = (None,None,0,1,None,0) \n",
    "\n",
    "# approximate p value with 10000 simulations\n",
    "p0 = confidence_region.test(null_point=est0, alt_point=est_params, \n",
    "                            null_cone=null_cone, alt_cone=alt_cone,\n",
    "                            sims=int(1e4))\n",
    "print (\"Approximate p-value of no migration:\", p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **ConfidenceRegion.test()** returns 0 if the likelihood ratio is more extreme than all 10000 null simulations performed.\n",
    "\n",
    "Also note that the above test involved **testing after model selection** (since we fixed the pulse time and \"nea\" join time).  \n",
    "This may be statistically invalid; the p-values should be checked and adjusted by simulation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
