{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import momi, pandas, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **`help()`** function to view documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package momi:\n",
      "\n",
      "NAME\n",
      "    momi\n",
      "\n",
      "DESCRIPTION\n",
      "    momi (MOran Models for Inference) is a python package for computing the site frequency spectrum,\n",
      "    a summary statistic commonly used in population genetics, and using it to infer demographic history.\n",
      "    \n",
      "    Please refer to examples/tutorial.ipynb for usage & introduction.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    compute_sfs\n",
      "    convolution\n",
      "    data_structure\n",
      "    demography\n",
      "    likelihood\n",
      "    math_functions\n",
      "    moran_model\n",
      "    parse_ms\n",
      "    size_history\n",
      "    tensor\n",
      "    util\n",
      "\n",
      "FILE\n",
      "    /home/snackattack/anaconda3/lib/python3.5/site-packages/momi/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(momi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating demographies\n",
    "\n",
    "momi uses a syntax based on the program ms by Richard Hudson.\n",
    "A demography is specified as a sequence of events.\n",
    "Time is measured going backwards from the present (t=0) to the past (t>0).\n",
    "\n",
    "There are 4 kinds of events:\n",
    "* **('-en', t, i, N)**\n",
    "    * At time t, population i has its scaled population size set to N, and growth rate set to 0.\n",
    "* **('-eg', t, i, g)**\n",
    "    * At time t, population i has exponential growth rate g (so for s>t, $N(s) = N(t) e^{(s-t)  g}$)\n",
    "* **('-ej', t, i, j)**\n",
    "    * At time t, all lineages in population i move into j.\n",
    "* **('-ep', t, i, j, p_ij)**\n",
    "    * At time t, each lineage in i moves into j with probability p_ij.\n",
    "\n",
    "Note **-en,-eg,-ej** are flags from ms, while **-ep** replaces the flag **-es** in ms.\n",
    "By default, all parameters are scaled as in ms, but this can be adjusted.\n",
    "\n",
    "See **`help(momi.Demography)`** or **`help(momi.Demography.__init__)`** for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example demography\n",
    "\n",
    "Here we consider a concrete example. More examples can be found at [example_demographies.ipynb](files/example_demographies.ipynb).\n",
    "\n",
    "Unlike ms, populations can be labeled by arbitrary strings.  \n",
    "Here we label the sampled populations as **'chb'** and **'yri'**.  \n",
    "The demography also involves admixture with a third population, **'nea'**.\n",
    "\n",
    "By default, the parameters are scaled like in **ms**. The population sizes are assumed to be rescaled by a \"reference\" size N_ref (e.g. 10,000), and time is scaled so there are 4*N_ref generations per unit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the list of events\n",
    "events = [('-en', 0., 'chb', 10.),         # at present (t=0), 'chb' has diploid population size 10 * N_ref\n",
    "          ('-eg', 0, 'chb' , 6.),          # at present (t=0), 'chb' growing at rate 6\n",
    "          ('-ep', .25, 'chb', 'nea', .03), # at t=.25, 'chb' has a bit of admixture from 'nea'\n",
    "          ('-ej', .5, 'chb', 'yri'),       # at t=.5, 'chb' joins onto 'yri' \n",
    "          ('-ej', 1.5, 'yri', 'nea'),      # at t=1.5, 'yri' joins onto 'nea'\n",
    "          ]\n",
    "\n",
    "# construct the Demography object, sampling 14 alleles from 'yri' and 10 alleles from 'chb'\n",
    "demo = momi.make_demography(events, sampled_pops=('yri','chb'), sampled_n=(14,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coalescent statistics\n",
    "\n",
    "`momi` can compute coalescent statistics such as the TMRCA (time to most recent common ancestor) and total branch length of the genealogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected TMRCA of all samples: \t 1.4192051765290294\n",
      "Expected TMRCA of chb samples: \t 1.2537429545633436\n",
      "Expected total branch length: \t 7.939637434148016\n"
     ]
    }
   ],
   "source": [
    "eTmrca = momi.expected_tmrca(demo)\n",
    "print(\"Expected TMRCA of all samples:\", \"\\t\", eTmrca)\n",
    "\n",
    "eTmrca_chb = momi.expected_deme_tmrca(demo, 'chb')\n",
    "print(\"Expected TMRCA of chb samples:\", \"\\t\", eTmrca_chb)\n",
    "\n",
    "eL = momi.expected_total_branch_len(demo)\n",
    "print (\"Expected total branch length:\", \"\\t\", eL)\n",
    "\n",
    "# See help(momi.expected_tmrca), etc. for more details.\n",
    "# Advanced users can use momi.expected_sfs_tensor_prod()\n",
    "# to compute these and many other summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Sample Frequency Spectrum (SFS)\n",
    "\n",
    "The expected SFS for configuration $((a_0,d_0),(a_1,d_1),...)$ is the expected number of SNPs with $a_0$ ancestral and $d_0$ derived alleles in population 0, $a_1$ ancestral and $d_1$ derived alleles in population 1, etc.\n",
    "\n",
    "The following all create a configuration with 13 ancestral, 1 derived alleles in \"yri\", and 10 ancestral, 0 derived alleles in \"chb\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **`momi.Configs`** to represent a list of configs, and **`momi.expected_sfs()`** to compute their expected SFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.6309565   0.95647102  0.0048574   0.06108049  0.04371188  0.00323204]\n"
     ]
    }
   ],
   "source": [
    "# a list of configs (index0 == yri, index1 == chb)\n",
    "configs = [( (14,0), (9,1), ), # 1 derived allele in chb\n",
    "           ( (13,1), (10,0), ), # 1 derived allele in yri \n",
    "           ( (11,3), (9,1),) , # 3 derived in yri, 1 derived in chb\n",
    "           ( (14,0), (0,10), ), # 0 derived in yri, all derived in chb\n",
    "           ( (2,12), (10,0), ), # 12 derived in yri, 0 derived in chb \n",
    "           ( (2,12), (2,8), ), # 12 derived in yri, 8 derived in chb\n",
    "          ]\n",
    "\n",
    "configs = momi.Configs((\"yri\",\"chb\"), configs)\n",
    "\n",
    "# the SFS entries corresponding to each config in configs\n",
    "eSFS = momi.expected_sfs(demo, configs, mut_rate=1.0)\n",
    "print (eSFS)\n",
    "\n",
    "# See help(momi.expected_sfs) for more options (e.g. folded SFS, sampling error, normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segregating sites\n",
    "\n",
    "A dataset of segregating sites has been stored in [tutorial_data.txt](tutorial_data.txt). The file is organized as follows:\n",
    "* Each locus starts with a line \"**//**\".\n",
    "* Subsequent lines correspond to segregating sites.\n",
    "    * The first column is the **position** of the site $x \\in [0,1]$.\n",
    "    * Subsequent columns give the **ancestral,derived** allele counts (respectively), in each population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yri\tchb\r\n",
      "\r\n",
      "//\r\n",
      "\r\n",
      "14,0\t6,4\r\n",
      "14,0\t9,1\r\n",
      "13,1\t10,0\r\n",
      "14,0\t9,1\r\n",
      "1,13\t10,0\r\n",
      "14,0\t0,10\r\n"
     ]
    }
   ],
   "source": [
    "# print first few lines in the shell\n",
    "!head tutorial_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`momi.read_seg_sites`** reads the file, and returns a **`momi.SegSites`** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# read the file with momi\n",
    "data_filename = \"tutorial_data.txt\"\n",
    "with open(data_filename,'r') as f:\n",
    "    seg_sites = momi.read_seg_sites(f)\n",
    "print (seg_sites.n_loci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`momi.SegSites`** has methods **`SegSites.position()`**, **`SegSites.config()`**, and **`SegSites.allele_count()`** to access the positions and allele counts at different sites.\n",
    "\n",
    "**`SegSites.position_arrays`** and **`SegSites.config_arrays`** are read-only arrays of the positions and configs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observed SFS \n",
    "\n",
    "**`momi.Sfs`** represents the observed SFS, i.e. the empirical frequencies of each config.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the Sfs for the observed data\n",
    "sfs = seg_sites.sfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Sfs.freq()`** returns the per-locus or total frequency of a config.    \n",
    "**`Sfs.loci`** and **`Sfs.total`** are read-only dicts of the per-locus and total frequencies, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9830\n",
      "438\n",
      "Unique configs: 162\n"
     ]
    }
   ],
   "source": [
    "print (sfs.freq(  ((13,1) , (10,0))  )) # equivalent to sfs.total[((13,1),(10,0))]\n",
    "print (sfs.freq( ((13,1) , (10,0)) , locus=0)) # equivalent to sfs.loci[0][((13,1),(10,0))]\n",
    "\n",
    "print (\"Unique configs:\", len(sfs.total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite likelihood\n",
    "\n",
    "The composite likelihood treats each site as independent.  \n",
    "`momi` provides 2 composite likelihoods:\n",
    "* **multinomial**: Fixed number of sites, each drawn from multinomial\n",
    "* **Poisson random field**: Random number of sites, drawn from Poisson distribution\n",
    "\n",
    "Here we illustrate the **multinomial** likelihood (the default). See **help(momi.composite_log_likelihood)** for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite log likelihood: -208826.992891\n"
     ]
    }
   ],
   "source": [
    "print (\"Composite log likelihood:\", momi.SfsLikelihoodSurface(sfs).log_likelihood(demo))\n",
    "# momi.composite_log_likelihood(seg_sites, demo) also works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "`momi` uses the package **`autograd`** to automatically compute derivatives.\n",
    "\n",
    "Gradients can be extremely useful in parameter inference.\n",
    "However, computing gradients is *not* necessary for basic functionality.\n",
    "Users who don't plan to use auto-differentiation can skip this section, or return to it later.\n",
    "\n",
    "To start, define a function that maps some parameters to a demography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "import autograd.numpy as np ## thinly wrapped version of numpy for auto-differentiation\n",
    "\n",
    "def demo_func(N_chb_bottom, N_chb_top, pulse_t, pulse_p, ej_chb, ej_yri):\n",
    "    ej_chb = pulse_t + ej_chb\n",
    "    ej_yri = ej_chb + ej_yri\n",
    "    \n",
    "    # use autograd.numpy for math functions (e.g. logarithm)\n",
    "    # This will allow us to take derivatives later\n",
    "    G_chb = -np.log(N_chb_top / N_chb_bottom) / ej_chb\n",
    "    \n",
    "    events = [('-en', 0., 'chb', N_chb_bottom),\n",
    "              ('-eg', 0, 'chb' , G_chb),\n",
    "              ('-ep', pulse_t, 'chb', 'nea', pulse_p),\n",
    "              ('-ej', ej_chb, 'chb', 'yri'),\n",
    "              ('-ej', ej_yri, 'yri', 'nea'),\n",
    "              ]\n",
    "\n",
    "    return momi.make_demography(events, ('yri','chb'), (14,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **`autograd`** to compute arbitrary gradients.  \n",
    "For example, here we compute the gradient of the TMRCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: [10.0, 0.1, 0.25, 0.03, 0.25, 1.0]\n",
      "Expected TMRCA: 1.312777162184671\n",
      "Gradient:\n",
      "[0.004524816751745795, 0.55838496083536815, 0.28449869706692166, 3.9671067152725321, 0.83175299796269409, 0.13916390098743475]\n"
     ]
    }
   ],
   "source": [
    "# function mapping vector of parameters to the TMRCA of the corresponding demography\n",
    "def tmrca_func(params):\n",
    "    # equivalent to demo_func(params[0], params[1], params[2], ...)\n",
    "    demo = demo_func(*params)\n",
    "    # return expected TMRCA\n",
    "    return momi.expected_tmrca(demo)\n",
    "\n",
    "# use autograd.grad() to obtain the gradient function\n",
    "tmrca_grad = autograd.grad(tmrca_func)\n",
    "\n",
    "x = [10., .1, .25, .03, .25, 1.]\n",
    "print (\"Parameters:\", x)\n",
    "print (\"Expected TMRCA:\", tmrca_func(x))\n",
    "print (\"Gradient:\")\n",
    "print (tmrca_grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details about using `autograd` for automatic differentiation\n",
    "\n",
    "`autograd` uses the magic of *operator overloading* to compute derivatives automatically.\n",
    "\n",
    "Documentation for autograd can be found at https://github.com/HIPS/autograd/\n",
    "\n",
    "Some do's and don'ts for autograd (from the tutorial):\n",
    "* Do use\n",
    "    * Arithmetic operations `+,-,*,/,**`\n",
    "    * Most of `numpy`'s functions\n",
    "    * Most `numpy.ndarray` methods\n",
    "    * Some `scipy` functions\n",
    "    * Indexing and slicing of arrays `x = A[3, :, 2:4]`\n",
    "    * Explicit array creation from lists `A = np.array([x, y])`\n",
    "* Don't use\n",
    "    * Assignment to arrays `A[0,0] = x`\n",
    "    * Implicit casting of lists to arrays `A = numpy.sum([x, y])`, use `A = numpy.sum(np.array([x, y]))` instead.\n",
    "    * `A.dot(B)` notation (use `numpy.dot(A, B)` instead)\n",
    "    * In-place operations (such as `a += b`, use `a = a + b` instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We'll try to infer the following demography from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_params = [10., .1, .25, .03, .25, 1.]\n",
    "true_demo = demo_func(*true_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a new dataset with **`ms`** (or similar program, e.g. `scrm`, `macs`, `msprime`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the dataset with ms\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ms_path = os.environ[\"MS_PATH\"]\n",
    "except KeyError:\n",
    "    ms_path = None\n",
    "\n",
    "if ms_path is not None:\n",
    "    print (\"Generating the dataset with ms\")\n",
    "    \n",
    "    n_loci = 20\n",
    "    kb_per_locus = 500\n",
    "    mut_rate_per_kb, recom_rate_per_kb = 1.,1.\n",
    "    \n",
    "    seg_sites = momi.simulate_ms(ms_path, true_demo, \n",
    "                                 n_loci, kb_per_locus * mut_rate_per_kb,\n",
    "                                 seeds = (8444303657, 6969571397, 652167805),\n",
    "                                 additional_ms_params=\"-r %f %d\" % (kb_per_locus*recom_rate_per_kb,\n",
    "                                                                    int(kb_per_locus * 1000)))\n",
    "    sfs = seg_sites.sfs\n",
    "    \n",
    "    with open(data_filename,'w') as f:\n",
    "        momi.write_seg_sites(f, seg_sites)\n",
    "else:\n",
    "    print (\"No ms_path provided, using SFS previously stored in %s.\" % data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define (lower,upper) bounds on the parameter space\n",
    "bounds = [(.01, 100.),\n",
    "          (.01, 100.),\n",
    "          (.01,5.),\n",
    "          (1e-100,.25),\n",
    "          (.01,5.),\n",
    "          (.01,5.)]\n",
    "\n",
    "# pick a random start value for the parameter search\n",
    "import random\n",
    "lower_bounds, upper_bounds = [l for l,u in bounds], [u for l,u in bounds]\n",
    "start_params = [random.triangular(lower, upper, mode)\n",
    "                for lower, upper, mode in zip(lower_bounds, upper_bounds, [1, 1, 1, .1, 1,1,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the MCLE (max composite likelihood estimate) with **`momi.composite_mle_search()`**.\n",
    "\n",
    "By default, `momi.composite_mle_search()` assumes `demo_func` is differentiable with `autograd`, and uses the gradient in a hill-climbing algorithm.\n",
    "* If you don't want to use `autograd`, you can disable the gradient (i.e. jacobian) with:\n",
    "    * `momi.composite_mle_search(..., jac=False, ...)`\n",
    "* Conversely, `momi.composite_mle_search()` provides options for using the Hessian (second-order derivative), in addition to the gradient.\n",
    "    * See **`help(momi.composite_mle_search)`**.\n",
    "\n",
    "Beware that **`momi.composite_mle_search()`** is vulnerable to local maxima.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting on full data, with 162 unique entries and 70540 total SNPs\n",
      "iter 0: KL-Divergence([  2.29387089e+01   6.99648977e+01   2.29809418e+00   9.00930536e-02\n",
      "   1.00000000e-02   1.59498063e+00]) == 0.729958\n",
      "iter 5: KL-Divergence([  6.08218916e-01   5.54419971e+01   6.86152333e-01   0.00000000e+00\n",
      "   1.00000000e-02   4.33184334e-01]) == 0.151079\n",
      "iter 10: KL-Divergence([ 1.06920144  0.94970895  0.66227058  0.          0.01        0.45205164]) == 0.092052\n",
      "iter 15: KL-Divergence([  7.31532144e+00   2.57894831e-01   5.41527259e-01   6.84719657e-03\n",
      "   1.00000000e-02   1.46043524e-01]) == 0.022555\n",
      "iter 20: KL-Divergence([ 11.68278499   0.13236188   0.49793254   0.07422937   0.02583899\n",
      "   1.47162366]) == 0.015292\n",
      "iter 25: KL-Divergence([ 9.83101     0.11575849  0.25160044  0.01965113  0.25517124  1.34590662]) == 0.002599\n",
      "iter 30: KL-Divergence([ 9.73025031  0.10258488  0.25146883  0.03304396  0.24872048  1.05365274]) == 0.002249\n"
     ]
    }
   ],
   "source": [
    "surface = momi.SfsLikelihoodSurface(seg_sites, demo_func)\n",
    "mcle_search_result = surface.find_optimum(start_params, bounds = bounds, maxiter = 500, output_progress = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results:\n",
      "     fun: 0.0022467364321905035\n",
      "     jac: array([  3.30848526e-08,  -4.31192756e-06,  -7.44235164e-07,\n",
      "        -5.82052748e-06,   3.02491140e-06,   5.44569526e-08])\n",
      " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
      "    nfev: 176\n",
      "     nit: 35\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([ 9.88122957,  0.10179253,  0.2535187 ,  0.03316898,  0.24673107,\n",
      "        1.06531002])\n"
     ]
    }
   ],
   "source": [
    "# print info such as whether search succeeded, number function/gradient evaluations, etc\n",
    "print (\"Search results:\")\n",
    "print (mcle_search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated params:\n",
      "[ 9.88122957  0.10179253  0.2535187   0.03316898  0.24673107  1.06531002]\n"
     ]
    }
   ],
   "source": [
    "est_params = mcle_search_result.x\n",
    "print (\"Estimated params:\")\n",
    "print (est_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence regions and Hypothesis testing\n",
    "\n",
    "`momi` uses the **Limit of Experiments** to compute approximate confidence regions and hypothesis tests.  \n",
    "A good reference is *Asymptotic Statistics* by **A.W. van der Vaart**.  \n",
    "(Some readers may be familiar with the **Godambe Information**, which uses a special case of this theory.)\n",
    "\n",
    "If certain regularity conditions hold, then the confidence regions will have the correct coverage\n",
    "properties as the data $\\to \\infty$.  \n",
    "In practice it is important to check the coverage via simulation, because:\n",
    "* It is not well understood when the regularity conditions hold in demographic inference.\n",
    "* Even if the regularity conditions hold, the coverage will be inaccurate if there is not enough data.\n",
    "\n",
    "`momi` can construct the asymptotic confidence regions for 2 limiting regimes:\n",
    "* **\"long\"**: fixed number of loci, whose length $\\to \\infty$\n",
    "    * Treats the segregating sites along each locus as a time series\n",
    "    * The loci should be independent (unlinked), but don't have to be identically distributed\n",
    "* **\"many\"**: many short loci, with the number of loci ` $\\to \\infty$\n",
    "    * The loci should be independent, and roughly identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a confidence region around the estimated parameters\n",
    "confidence_region = momi.ConfidenceRegion(est_params, demo_func, seg_sites, regime=\"long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value at the truth, using likelihood ratio test.  \n",
    "Asymptotically, the log-likelihood ratio is a transformation of a Gaussian.  \n",
    "We use **`ConfidenceRegion.test()`** to compute an approximate p-value, by an inexpensive simulation of 1000 Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value at true params: 0.388\n"
     ]
    }
   ],
   "source": [
    "print (\"p-value at true params:\", confidence_region.test(true_params, sims=int(1e3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ConfidenceRegion.test()`** can also compute the p-values at a list of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.059,  0.898,  0.079])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence_region.test([est_params * .95, est_params * .98, est_params * 1.05], sims=int(1e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ConfidenceRegion.test()`** can also construct tests and intervals based on the Wald-statistic.  \n",
    "This does not require simulation, but typically the Wald statistic is less powerful and converges more slowly than the likelihood ratio.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value at true params (Wald): 0.784551512519462\n"
     ]
    }
   ],
   "source": [
    "print (\"p-value at true params (Wald):\", confidence_region.test(true_params, test_type=\"wald\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of Wald is the ease of computing marginal intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% marginal confidence intervals\n",
      "[[  8.88067209  10.88178705]\n",
      " [  0.08845577   0.11512929]\n",
      " [  0.22893113   0.27810626]\n",
      " [  0.02192726   0.04441071]\n",
      " [  0.21920288   0.27425926]\n",
      " [  0.77538396   1.35523607]]\n"
     ]
    }
   ],
   "source": [
    "print (\"95% marginal confidence intervals\")\n",
    "print (confidence_region.wald_intervals(lower=.025,upper=.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex hypothesis tests\n",
    "\n",
    "**`ConfidenceRegion.test()`** can also perform more complicated tests for nested models.  \n",
    "We consider testing that the pulse probability is 0.  \n",
    "We will also fix the pulse time and \"nea\" join time; otherwise the demography is not identifiable, which violates a crucial regularity condition.  \n",
    "(If the pulse probability is 0, then the pulse time, and the \"nea\" join on time, have no affect on the likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting on full data, with 162 unique entries and 70540 total SNPs\n",
      "iter 0: KL-Divergence([ 9.88141584  0.17405436  0.2467009 ]) == 0.021100\n",
      "iter 5: KL-Divergence([ 8.47502433  0.19269352  0.28584456]) == 0.020159\n"
     ]
    }
   ],
   "source": [
    "# Find the MLE in the null model space\n",
    "start0, bound0 = list(est_params), list(bounds)\n",
    "start0[2] = bound0[2] = est_params[2] # fix the pulse time\n",
    "start0[5] = bound0[5] = est_params[5] # fix the 'nea' join time\n",
    "start0[3] = bound0[3] = 1e-100 # set the pulse probability to 0\n",
    "\n",
    "constrained_opt_result = surface.find_optimum(start0, bounds = bound0, maxiter = 500, output_progress = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE of null model:\n",
      "[  8.48933350e+000   1.92703032e-001   2.53518698e-001   1.00000000e-100\n",
      "   2.85838381e-001   1.06531002e+000]\n"
     ]
    }
   ],
   "source": [
    "print (\"MLE of null model:\")\n",
    "est0 = constrained_opt_result.x\n",
    "print (est0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate p-value of no migration: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Null model:\n",
    "# parameters 0,1,4 unconstrained (\"None\")\n",
    "# parameters 2,3,5 are fixed (\"0\")\n",
    "null_cone = (None,None,0,0,None,0)\n",
    "\n",
    "# Alternative model:\n",
    "# parameters 0,1,4, unconstrained (\"None\")\n",
    "# parameters 2,5 fixed (\"0\")\n",
    "# parameter 3 bounded on the left (\"1\". use \"-1\" if bounded on the right)\n",
    "alt_cone = (None,None,0,1,None,0) \n",
    "\n",
    "# approximate p value with 10000 simulations\n",
    "p0 = confidence_region.test(null_point=est0, alt_point=est_params, \n",
    "                            null_cone=null_cone, alt_cone=alt_cone,\n",
    "                            sims=int(1e4))\n",
    "print (\"Approximate p-value of no migration:\", p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **ConfidenceRegion.test()** returns 0 if the likelihood ratio is more extreme than all 10000 null simulations performed.\n",
    "\n",
    "Also note that the above test involved **testing after model selection** (since we fixed the pulse time and \"nea\" join time).  \n",
    "This may be statistically invalid; the p-values should be checked and adjusted by simulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
